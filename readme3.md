# 生产级RAG系统：基于动态文件加载与答案溯源

本项目是RAG系列教程的最终演进版本，实现了一个功能完备、结构清晰、可用于实际场景的检索增强生成（RAG）问答系统。它能动态地从本地目录加载多种格式的文档（如TXT, PDF, Word, Markdown等），构建可随时扩展的知识库，并提供带有明确来源的、可追溯的答案。

## 核心功能与优势

- **动态知识库**: 无需在代码中硬编码知识内容。只需将您的文档（`.txt`, `.pdf`, `.docx`, `.md`, `.xlsx`等）放入指定的`./docs`目录，系统便会自动加载。
- **多文件格式支持**: 内置了对多种常见文件格式的解析能力，极大地方便了知识库的构建。
- **智能文本分块**: 采用了更健壮的文档分块策略，优先在句子末尾进行切分，最大程度地保留了文本的语义完整性。
- **答案可溯源**: 系统在生成答案时，提供的上下文中会明确标注每段信息的具体文件来源。这使得最终答案的依据清晰可见，增强了可信度。
- **完全模块化设计**: 所有核心逻辑被封装在`RAGSystemFromFile`类中，代码高度解耦，易于维护、测试和扩展。
- **高效持久化**: 自动缓存处理后的一切数据，包括模型、文本块、向量索引和文件映射关系，二次启动时可实现秒级加载。
- **清晰的执行日志**: 整个流程，从文件加载到最终答案的生成，都有详细的中文日志输出，便于理解和调试。

## 项目结构

```
.
├── main3.py                    # 主程序脚本
├── file_utils.py               # 文件加载工具模块
├── readme3.md                  # 本项目说明文件
├── docs/                       # 存放你的知识库文档 (需手动创建)
│   ├── example.txt
│   └── another_doc.pdf
├── local_m3e_model/            # (自动生成) 本地缓存的嵌入模型
├── m3e_faiss_file_index.bin    # (自动生成) 基于文本块的Faiss索引文件
└── chunks_file_mapping.npy     # (自动生成) 包含所有块和映射关系的数据文件
```

## 安装依赖

在运行此项目之前，您需要安装所有必需的Python库。由于涉及多种文件格式的读取，依赖项比前序版本要多。

```bash
# 核心依赖
pip install sentence-transformers faiss-cpu numpy openai

# 文件读取所需依赖
pip install PyPDF2 python-docx pandas openpyxl markdown beautifulsoup4
```
**注意**:
- `faiss-cpu`是CPU版本的Faiss。如果您的机器支持并配置了CUDA，可以安装`faiss-gpu`以获得更好的性能。
- 请确保已安装所有文件读取相关的库，否则对应类型的文件将无法被加载。

## 配置与使用

### 1. 准备知识库

在项目根目录下创建一个名为 `docs` 的文件夹。将您希望作为知识库的所有文档文件放入该文件夹中。

### 2. 设置API密钥

本项目需要使用智谱AI（ZhipuAI）的API服务。请在运行脚本前，设置一个名为 `ZHIPUAI_API_KEY` 的环境变量。

**在 Linux 或 macOS 上:**
```bash
export ZHIPUAI_API_KEY="你的智谱AI_API_KEY"
```

**在 Windows 上:**
```powershell
$env:ZHIPUAI_API_KEY="你的智谱AI_API_KEY"
```

### 3. 运行系统

完成以上步骤后，直接在终端运行主脚本即可：

```bash
python main3.py
```

- **首次运行**: 系统会执行完整的初始化流程：加载`docs`目录下的所有文件，下载嵌入模型，对文档进行分块，生成嵌入向量，构建并保存Faiss索引和映射文件。这个过程可能需要一些时间，具体取决于您的文档数量和大小。
- **后续运行**: 系统会直接加载本地缓存的文件，启动速度将非常快。

**重要提示**: 当前实现为了简化，并不会自动检测`docs`目录中文件的变化。如果您修改、添加或删除了知识库文件，需要手动删除`m3e_faiss_file_index.bin`和`chunks_file_mapping.npy`这两个缓存文件，然后重新运行`main3.py`来强制重建索引。

## 代码逻辑深度解析

`main3.py`将整个RAG流程封装在`RAGSystemFromFile`类中，其核心逻辑分为初始化和问答两个阶段。

### 1. 初始化 (`__init__`)

当`RAGSystemFromFile`被实例化时，会按顺序执行以下关键步骤：

1.  **加载文档 (`_load_documents`)**:
    -   调用`file_utils.py`中的`load_documents_from_directory`函数，遍历`./docs`目录。
    -   根据文件后缀名，使用对应的解析器（如PDF阅读器、Word阅读器等）来提取纯文本内容。
    -   妥善处理加载过程中可能出现的错误，并在日志中报告。
    -   如果`docs`目录为空或无法加载任何文件，则会加载一组默认的示例文档作为备用。

2.  **加载嵌入模型 (`_load_model`)**:
    -   与前序版本相同，检查本地是否存在`m3e-base`模型，若无则从Hugging Face下载。

3.  **创建/加载索引 (`_create_or_load_index_and_mappings`)**:
    -   这是整个系统的核心准备步骤。当检测到没有本地缓存的索引时，它会：
        -   **分块**: 对所有加载的文档内容执行智能分块。
        -   **生成嵌入**: 为所有生成的文本块计算向量嵌入。
        -   **构建索引**: 将这些嵌入向量存入Faiss索引。
        -   **保存一切**: 将Faiss索引、所有文本块、文档与块的映射关系，以及**文档的来源信息列表 (`doc_sources`)** 全部打包并保存到本地文件中。保存来源信息是实现答案溯源的关键。

4.  **初始化LLM客户端 (`_initialize_openai_client`)**:
    -   使用环境变量中的API密钥创建与大模型服务的连接。

### 2. 问答流程 (检索与生成)

当用户输入问题后，系统执行以下流程：

1.  **检索 (`retrieve` 方法)**:
    -   将用户的查询转换为向量，然后在Faiss索引中搜索最相似的**文本块**。
    -   **溯源核心**:
        -   根据检索到的块ID，从缓存的映射数据中找到它们所属的**原始文档ID**以及**原始文件名**。
        -   返回两组带有来源信息的数据：
            1.  去重后的原始文档列表，每个元素是 `(文档内容, 文件名)`。
            2.  按相关性排序的文本块列表，每个元素是 `(文档ID, 块内容, 文件名)`。

2.  **生成 (`generate_answer` 方法)**:
    -   **构建可溯源的上下文**: 这是本项目的最大亮点。它会创建一个结构化、信息极其丰富的上下文，格式如下：

        ```
        --- 相关原始文档 ---
        【文档1 | 来源: a.pdf】
        [a.pdf的完整内容]

        【文档2 | 来源: b.docx】
        [b.docx的完整内容]

        --- 高度相关的文本块 ---
        【文本块1 | 来源: a.pdf】
        [从a.pdf中检索到的与问题最相关的文本块内容]

        【文本块2 | 来源: b.docx】
        [从b.docx中检索到的与问题最相关的文本块内容]
        ```
    -   将这个带有清晰来源标注的上下文和用户问题一起发送给LLM。这不仅能帮助LLM更好地理解问题，也使得最终的答案完全透明、可验证。 